{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24560ceb",
   "metadata": {},
   "source": [
    "Day 1: Ingest and Index Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1906fb87-46c6-4070-8dc4-2fe8b9979a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Implementation. Let's now put everything together into a reusable function:\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e82e61de-8fbd-40f1-937e-dae912f540e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# We can now use this function for different repositories:\n",
    "\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da727f30",
   "metadata": {},
   "source": [
    "DAY 2 : Chunking and Intelligent Processing for Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac5c10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"sliding window\" method. This method creates overlapping chunks of text, which can help preserve context across chunk boundaries.\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169fcdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process all the documents:\n",
    "if 'evidently_docs' not in globals():\n",
    "    evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489fe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting by Paragraphs and Sections\n",
    "\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "\n",
    "# use regex to split by headers\n",
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# If we want to split by second-level headers, that's what we do:\n",
    "sections = split_markdown_by_level(text, level=2)\n",
    "\n",
    "#Now we iterate over all the docs to create the final result:\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f560d9e-c88f-48af-951b-60c9436100c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key here or ensure it's set in your environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e13879-5d6e-4692-9e8d-753bb11bd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39e03d61-08e4-4ea8-8ab6-373f505627ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for intelligent chunking:\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195e129-b27a-44d4-a48b-83ebda2858fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we apply this to every document:\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728a966",
   "metadata": {},
   "source": [
    "Day 3: Add Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3057b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared the docs\n",
    "if 'read_repo_data' not in globals():\n",
    "    raise NameError(\"Function 'read_repo_data' is not defined. Please run the cell that defines it first.\")\n",
    "\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3359487b-5035-45a3-af9c-a8c32a384da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index this data with minsearch\n",
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e0a919-8a2c-4e6b-8503-088c0b365c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '8bfd724e4f', 'question': \"Compilation Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)  'NoneType' object is not iterable\", 'sort_order': 38, 'content': 'In the macro `test_accepted_values` (found in `tests/generic/builtin.sql`), an error was triggered by the test `accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_` located in `models/staging/schema.yml`.\\n\\nTo resolve this issue, ensure the following variable is added to your `dbt_project.yml` file:\\n\\n```yaml\\nvars:\\n  payment_type_values: [1, 2, 3, 4, 5, 6]\\n```', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/038_8bfd724e4f_compilation-error-in-test-accepted_values_stg_gree.md'}, {'id': 'c180431de3', 'question': 'DBT Cloud production error: prod dataset not available in location EU', 'sort_order': 4, 'content': \"**Problem:**\\n\\nI am trying to deploy my DBT models to production using DBT Cloud. The data should reside in BigQuery with the dataset location as EU. However, when running the model in production, a prod dataset is incorrectly created in BigQuery with a location of US. This leads to the build failing with the error:\\n\\n```\\nERROR 404: project.dataset:prod not available in location EU\\n```\\n\\nI have attempted various fixes, but I'm unsure if there is a simpler solution than creating my projects or buckets in the US location.\\n\\nNote: Everything functions properly in development mode; the issue arises only during job scheduling and execution in production.\\n\\n**Solution:**\\n\\n1. Manually create the `prod` dataset in BigQuery with the EU location specified.\\n2. Rerun the production job.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/004_c180431de3_dbt-cloud-production-error-prod-dataset-not-availa.md'}, {'id': '9f9a1b9e4f', 'question': 'Terraform: Teardown of BigQuery Dataset', 'sort_order': 126, 'content': \"When running `terraform destroy`, the following error can occur:\\n\\n```\\nDo you really want to destroy all resources?\\n\\nTerraform will destroy all your managed infrastructure, as shown above.\\n\\nThere is no undo. Only 'yes' will be accepted to confirm.\\n\\nEnter a value: yes\\n\\ngoogle_bigquery_dataset.homework_dataset: Destroying... [id=projects/terraform-demo-449214/datasets/homework_dataset]\\n\\n╷\\n\\n│ Error: Error when reading or editing Dataset: googleapi: Error 400: Dataset terraform-demo-449214:homework_dataset is still in use, resourceInUse\\n```\\n\\nThis is because the dataset is still in use by a table. To delete the dataset, set the `delete_contents_on_destroy` property to `true` in the `main.tf` file.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/126_9f9a1b9e4f_terraform-teardown-of-bigquery-dataset.md'}, {'id': '04f55d5846', 'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6', 'sort_order': 32, 'content': '1. Go to **Account settings** > **Project** > **Analytics**.\\n2. Click on your connection.\\n3. Scroll down to **Location** and type in the GCP location exactly as displayed in GCP (e.g., `europe-west6`). You might need to reupload your GCP key.\\n\\n4. Delete your dataset in Google BigQuery (GBQ).\\n5. Rebuild the project using the command:\\n   \\n   ```bash\\n   dbt build\\n   ```\\n\\n6. The newly built dataset should be in the correct location.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/032_04f55d5846_bigquery-adapter-404-not-found-dataset-was-not-fou.md'}, {'id': 'e7738f47c8', 'question': 'Should I pay for cloud services?', 'sort_order': 28, 'content': \"It's not mandatory. You can take advantage of their free trial.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/028_e7738f47c8_should-i-pay-for-cloud-services.md'}, {'id': '05727a95dd', 'question': 'Homework and Leaderboard: What is the system for points in the course management', 'sort_order': 18, 'content': \"After you submit your homework, it will be graded based on the number of questions in that particular assignment. You can see the number of points you have earned at the top of the homework page. Additionally, in the [leaderboard](https://courses.datatalks.club/de-zoomcamp-2025/leaderboard), you will find the sum of all points you've earned: points for Homeworks, FAQs, and Learning in Public.\\n\\nPoint System Overview:\\n\\n- **Homework:** Points vary by assignment based on the number of questions.\\n- **FAQ Contribution:** You get a maximum of 1 point for contributing to the FAQ in the respective week.\\n- **Learning in Public:** For each learning in public link, you earn one point. You can achieve a maximum of 7 points.\\n\\nCheck this [video](https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce) for more details.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/05727a95dd_homework-and-leaderboard-wha.md'}, {'id': 'c18119ba32', 'question': 'Invalid dataset ID Error when running the gcp_setup flow', 'sort_order': 10, 'content': 'When following the [YouTube lesson](https://www.youtube.com/watch?v=nKqjjLJ7YXs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23) and then running the [gcp_setup flow](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/flows/05_gcp_setup.yaml), the error occurs during the `create_bq_dataset` task.\\n\\nThe error is less clear, but it stems from using a dash in the dataset name. To resolve this, change the dataset name to something like \"de_zoomcamp\" to avoid using a dash. This should resolve the error.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/010_c18119ba32_invalid-dataset-id-error-when-running-the-gcp_setu.md'}, {'id': '0f6e2d3813', 'question': 'How do I solve the Dbt Cloud error: prod was not found in location?', 'sort_order': 5, 'content': \"You might encounter this error when trying to run dbt in production after following the instructions in the video ‘DE Zoomcamp 4.4.1 - Deployment Using dbt Cloud (Alternative A’):\\n\\n```\\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nNot found: Dataset module-4-analytics-eng:prod was not found in location europe-west10\\n```\\n\\nThis error is easily resolved. Here are two solutions to address this issue:\\n\\n**Solution #1: Matching the dataset's data location with the source dataset**\\n\\n- Set your ‘prod’ dataset's data location to match the data location of your ‘trips_data_all’ dataset in BigQuery. The dbt process works for the instructor because her ‘prod’ dataset is in the same region as her source data. Since your ‘trips_data_all’ is in europe-west10 (or another region besides US), your prod needs to be there too, not US (which is the default setting when dbt creates a dataset for you in BigQuery).\\n\\n**Solution #2: Changing the dataset to <development dataset>**\\n\\n1. Go to: Deploy / Environments / Production (your production environment) / Settings.\\n2. Look at the Deployment credentials. There is an input field called Dataset. The input of ‘prod’ is likely here.\\n3. Replace ‘prod’ with the name of the Dataset that you worked with during development (before moving to Production). This is the Dataset name inside your BigQuery where you successfully ran ‘dbt debug’ and ‘dbt build’.\\n4. After saving, you are ready to rerun your Job!\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/005_0f6e2d3813_how-do-i-solve-the-dbt-cloud-error-prod-was-not-fo.md'}, {'id': 'd7abe28e77', 'question': 'Which set-up should I use for my dlt homework?', 'sort_order': 2, 'content': 'Technically you can use any code editor or Jupyter Notebook, as long as you can run dbt and answer the homework questions. A lot of code is provided by the instructor on the homework page to give you a head start in the right direction: [dlt Homework Instructions](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2025/workshops/dlt/dlt_homework.md).\\n\\nThe most practical way is to use the provided Colabs Jupyter notebook called ‘dlt - Homework.ipynb’ which you can find here: [Colab Notebook](https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7#scrollTo=BtsSxtFfXQs3) since all of the provided code is applicable in the Colabs set-up.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/002_d7abe28e77_which-set-up-should-i-use-for-my-dlt-homework.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}]\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "results = faq_index.search(query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2077c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.7.1+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.9)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d94cce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For vector search, we need to turn our documents into vectors (embeddings).\n",
    "# We will use the sentence-transformers library for this purpose.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Let's take one document from the FAQ dataset:\n",
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n",
    "\n",
    "# We combine the question and answer text, then convert it to an embedding vector.\n",
    "# Let's do the same for the query:\n",
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n",
    "\n",
    "# This is how we compute similarity between the query and document vectors:\n",
    "similarity = v_query.dot(v_doc) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4efcb85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:39<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(449, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# turn our docs into embeddings\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n",
    "print(faq_embeddings.shape)  # should be (number_of_docs, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "643b688c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x7f2dc544c050>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's use VectorSearch:\n",
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b145ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa68b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [05:02<00:00,  1.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x7f2dbf29f710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first create an embedding for our query (q), then search for similar document embeddings.\n",
    "# You can easily do the same with the Evidently docs (but only use the chunk field for embeddings):\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc68b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid search - combines both exact keyword matches and vector search.\n",
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e61527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting this together. But before we can use it in our agent, we need to organize the code. Let's put all the code into different functions:\n",
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "024dba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}]\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index, VectorSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Make sure de_dtc_faq is defined\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "\ttext_fields=[\"question\", \"content\"],\n",
    "\tkeyword_fields=[]\n",
    ")\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# Load embedding model if not already loaded\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Create embeddings for FAQ documents\n",
    "faq_embeddings = []\n",
    "for d in de_dtc_faq:\n",
    "\ttext = d['question'] + ' ' + d['content']\n",
    "\tv = embedding_model.encode(text)\n",
    "\tfaq_embeddings.append(v)\n",
    "faq_embeddings = np.array(faq_embeddings)\n",
    "\n",
    "# Create VectorSearch index\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n",
    "\n",
    "# Now we can use hybrid_search() to get results from both methods:\n",
    "query = 'Can I join the course now?'\n",
    "results = hybrid_search(query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e289dd",
   "metadata": {},
   "source": [
    "Day 4: Agents and Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a19648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY present, prefix: sk-pro...\n"
     ]
    }
   ],
   "source": [
    "# ...replace the existing OpenAI setup cell with this...\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load variables from .env (if present) into environment\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. Create a .env or set the env var before running.\"\n",
    "    )\n",
    "\n",
    "# Create client explicitly with the key\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Safe verification (only shows a short prefix)\n",
    "print(\"OPENAI_API_KEY present, prefix:\", OPENAI_API_KEY[:6] + \"...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc15187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)   # should point to /.../.venv/.../python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fbc6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True prefix: sk-pro...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "k = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Key loaded:\", bool(k), \"prefix:\", (k[:6] + \"...\") if k else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c6fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It depends on the course's policies and schedule. Many courses have specific enrollment periods or prerequisites. I recommend checking the course website or contacting the instructor or administration for more details on joining at this point.\n"
     ]
    }
   ],
   "source": [
    "# try asking a question without giving the LLM access to search:\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad4f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calling\n",
    "# describe this function  using a special description format\n",
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657493ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"query\":\"Can I join the course now?\"}', call_id='call_z96IsqOjqpRI8Mr8RkzOyzUF', name='text_search', type='function_call', id='fc_68dac2a3f62081919b8cb721525d6ba90c1ee7902699ba09', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a62bd84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faq_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m call = response.output[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m arguments = json.loads(call.arguments)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m result = \u001b[43mtext_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m call_output = {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction_call_output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcall_id\u001b[39m\u001b[33m\"\u001b[39m: call.call_id,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(result),\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# extending the chat_messages list and sending the entire conversation history back to the LLM\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtext_search\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> List[Any]:\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    Perform a text-based search on the FAQ index.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m        List[Any]: A list of up to 5 search results returned by the FAQ index.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfaq_index\u001b[49m.search(query, num_results=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'faq_index' is not defined"
     ]
    }
   ],
   "source": [
    "#  invoke the function with these arguments:\n",
    "import json\n",
    "\n",
    "if 'response' not in globals():\n",
    "    raise NameError(\"Variable 'response' is not defined. Please run the cell that creates 'response' first.\")\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n",
    "\n",
    "# extending the chat_messages list and sending the entire conversation history back to the LLM\n",
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20c4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt\n",
    "# system prompt is very important: it influences how the agent behaves\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "# if we want the agent to make multiple search queries, we can modify the prompt:\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2dbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic AI\n",
    "# A library to create AI agents using Pydantic models\n",
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cc9225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now define an agent with Pydantic AI and give it the text_search tool:\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6291808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure faq_index is defined before running the agent\n",
    "from minsearch import Index\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "\ttext_fields=[\"question\", \"content\"],\n",
    "\tkeyword_fields=[]\n",
    ")\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# Now run the agent\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1671d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output=\"Yes, you can still join the course even after the start date. You are eligible to submit homework, but keep in mind that there will be deadlines for submitting assignments and final projects, so it's important not to procrastinate. \\n\\nFor future reference, the next cohort starts on January 13th, 2025. You can register before the course starts using the provided registration link. If you would like more details, feel free to ask!\")\n"
     ]
    }
   ],
   "source": [
    "# In Jupyter, use 'await' directly for async functions:\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
